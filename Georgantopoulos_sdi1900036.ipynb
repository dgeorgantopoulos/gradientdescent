{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WxNv4bWSnjF"
      },
      "source": [
        "Δημήτριος Γεωργαντόπουλος sdi1900036\n",
        "Εργασία 1η"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqWP5UZ2UaEl"
      },
      "source": [
        "καθώς τα import βρίσκονται στο πρώτο cell, σας παρακαλώ τρέξτε πρώτα αυτό πρίν"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXNWuCDodMV9"
      },
      "outputs": [],
      "source": [
        "!pip install numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "np.random.seed(36)\n",
        "X=np.random.randint(0,15,size=(3,4))\n",
        "print(\"X is:\\n\",X)\n",
        "Y=np.random.randint(0,15,size=(4,3))\n",
        "print(\"Y is:\\n\",Y)\n",
        "a=np.random.randint(0,15,size=4)\n",
        "print(\"a is:\\n\",a)\n",
        "b=np.random.randint(0,15,size=4)\n",
        "print(\"b is:\\n\",b)\n",
        "\n",
        "c=np.inner(a,b) #erwtima 1.1 ypologismos eswterikou\n",
        "print(\"inner product is:\",c)\n",
        "print(\"\\n\")\n",
        "\n",
        "d=X.dot(a) #1.2\n",
        "print(d)\n",
        "print(\"\\n\")\n",
        "e=np.dot(X,Y) #1.3\n",
        "print(e)\n",
        "print(\"\\n\")\n",
        "\n",
        "f=np.linalg.norm(a)\n",
        "print(\"h norma einai:\",f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ayj2ZUPYYKK3"
      },
      "source": [
        "# ΕΡΩΤΗΜΑ 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Το gradient descent αφορά μέθοδο εύρεσης των κατάλληλων τιμών των μεταβλητών μιας συνάρτησης, μέσω των οποίων αυτή λαμβάνει τις χαμηλότερες τιμές της.\n",
        "Οι μεταβλητές x1,x2 αποτελούν την αντίστοιχη μορφής της αρχικής συνάρτησης, αν αυτή τη παραγωγίσουμε προς x1 και x2 αντιστοίχως."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "u8O2cPELSjNX"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(x1,x2,ite,learnrate):\n",
        "  funct=np.array([])\n",
        "  for aIter in range(ite):\n",
        "    val=pow(x1-2,2)+pow(x2-3,2)\n",
        "    x1=x1-learnrate*(2*x1-4)\n",
        "    x2=x2-learnrate*(2*x2-6)\n",
        "    funct=np.append(funct,val)\n",
        "    print(\"we are in iteration No.:\",aIter)\n",
        "    print(\"function value is:\",x1,\",\",x2)\n",
        "  plt.plot(funct)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvOgnxRwOWKI"
      },
      "source": [
        "Όσον αφορά το learning rate 0,5 με 10 επαναλήψεις, ήδη από την 1η κιόλας επανάληψη οι x1 και x2 φτάνουν στις σωστές τιμές 2,3 αντιστοίχως."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRKqCc7-CKVm"
      },
      "outputs": [],
      "source": [
        "t1=0\n",
        "t2=0\n",
        "learn=0.5\n",
        "iter=10\n",
        "\n",
        "gradient_descent(t1,t2,iter,learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb8GfTmwPES0"
      },
      "source": [
        "Σε περίπτωση που έχουμε ιδιαίτερα υψηλό learning rate, διακρίνουμε πως και αυτό είναι αρκετά γρήγορο, λαμβάνοντας τιμές ιδιαίτερα κοντά στις επιθυμητές σε περίπου 6 επαναλήψεις (απόκλιση περί το 0,01). Παρ'όλα αυτά, οι τιμές προσεγγίζουν όλο και περισσότερο το 2,3 οσο περνάνε οι επαναλήψεις, όπως σε κάθε iteration παρατηρείται ιδιαίτερα μικρή διαφορά."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVHUj2LkCPs-"
      },
      "outputs": [],
      "source": [
        "t1=0\n",
        "t2=0\n",
        "learn=0.8\n",
        "iter=5000\n",
        "\n",
        "gradient_descent(t1,t2,iter,learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXBfFjz-QPAZ"
      },
      "source": [
        "Σε περίπτωση που έχουμε ιδιαίτερα μικρό learning rate, η ταχύτητα υπολογισμού των τιμών γίνεται εξαιρετικά μικρή, με τα x1,x2 μετά από 10.000 επαναλήψεεις να εμφανίζουν σημαντική απόκλιση σχεδόν 0,3 με 0,5 από τις τιμες (2,3). Χαρακτηριστικό είναι οτι μετά απο κάθε επανάληψη(πέραν από τις πρώτες περίπου 5), παρατηρείται εξαιρετικά μικρή αλλαγή στις τιμές των μεταβλητών. Παρ'όλα αυτά δεν παρουσιάζουν στασιμότητα, εφόσον με μεγάλο αριθμό iterations φτάνουν ολοένα και πιο κοντά στο (2,3)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsIcY-HpCQE7"
      },
      "outputs": [],
      "source": [
        "t1=0\n",
        "t2=0\n",
        "learn=0.0001\n",
        "iter=10000\n",
        "\n",
        "gradient_descent(t1,t2,iter,learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg15EzxDSiuC"
      },
      "source": [
        "Ακόμη κι αν αλλάξουμε τις αρχικές τμές των x1,x2 από 0,0, αυτές προφανώς και πάλι φτάνουν στο επιθυμητό (2,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HOrn_Z_SjIo"
      },
      "outputs": [],
      "source": [
        "t1=6\n",
        "t2=5\n",
        "learn=0.5\n",
        "iter=10\n",
        "\n",
        "gradient_descent(t1,t2,iter,learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSEi0xYNRFGe"
      },
      "source": [
        "ΣΥΝΆΡΤΗΣΗ F2\n",
        "\n",
        "Σε αντίθεση με την f1 που χρησιμοποιούσαμε σκέτους integers για την αποθήκευση των τιμών x1,x2, στη προκειμένη περίπτωση εξαιτίας των ιδιαίτερα υψηλών τιμών που υπολογίζονταν, αν δοκιμάζαμε πάλι τη χρήση integers προέκυπτε overflow error. Γι'αυτόν τον λόγο, θα αποθηκεύω πλέον τις τιμές σε array, το οποίο δεν έχει ιδιαίτερους περιορισμούς ως προς το μέγεθος της συμβολοσειράς η οποία θα αποθηκευτεί σε αυτόν."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e884MP2WPOUN"
      },
      "outputs": [],
      "source": [
        "def gradient_descent2(x1,x2,ite,learnrate):\n",
        "  funct=np.array([])\n",
        "  for aIter in range(ite):\n",
        "    val=(pow((1-(x2[aIter]-3)),2)+20*pow((x1[aIter]+3)-pow(x2[aIter]-3,2),2))\n",
        "    x1=np.append(x1,x1[aIter]-learnrate*(40*(x1[aIter]+3)-40*(pow(x2[aIter]-3,2))))\n",
        "    x2=np.append(x2,(x2[aIter]-learnrate*(2*x2[aIter]-8-80*(x1[aIter]+3)*(x2[aIter]-3)+80*pow(x2[aIter]-3,3))))\n",
        "    funct=np.append(funct,val)\n",
        "    print(\"we are in iteration No.:\",aIter)\n",
        "    print(\"function value is:\",x1[aIter+1],\",\",x2[aIter+1]) #prosthetw +1 efoson sthn prwth thesh tou array einai to 0\n",
        "  plt.plot(funct)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI9xBAHzXZ9W"
      },
      "source": [
        "Με το learning rate στο 0,5 αντιλαμβανόμαστε οτι τα x1,x2 δεν παίρνουν κάποια συγκεκριμένη σταθερή τιμή, ενώ μάλιστα μετά από 6 επαναλήψεις, λαμβάνουν τιμές εξαιρετικές μεγάλες για να τις χειριστεί η python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDoIMMT_GYRV"
      },
      "outputs": [],
      "source": [
        "t1=np.array([0])\n",
        "t2=np.array([0])\n",
        "learn=(0.5)\n",
        "T=10\n",
        "gradient_descent2(t1,t2,T,learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts_sxMKdO84Q"
      },
      "source": [
        "Σε περίπτωση εφαρμογής ακόμα μεγαλύτερου ρυθμού εκμάθησης, οι x1,x2 έχουν εξίσου απρόβλεπτα αποτελέσματα όπως και πριν, εφόσον πάλι γρήγορα φεύγουν και αυτές εκτός ορίων."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "illp65ArNwZU"
      },
      "outputs": [],
      "source": [
        "t1=np.array([0])\n",
        "t2=np.array([0])\n",
        "learn=0.8\n",
        "T=100\n",
        "gradient_descent2(t1,t2,T,learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TqefdA8Xana"
      },
      "source": [
        "Αν αξιοποιήσουμε εξαιρετικά μικρό learning rate, ακόμα και με μεγάλο αριθμό επαναλήψεων, δεν προκύπτει κάποιο πρόβλημα overflow. Παρ'όλα αυτά, οι τιμές των x1,x2 παραμένουν πάντοτε μεταβαλλόμενες, αλλά συγκλίνουν εν τέλει περί των τιμών -2 και 4 για τα x1 και x2 αντιστοίχως"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm8lUCX4Nw-B"
      },
      "outputs": [],
      "source": [
        "t1=np.array([0])\n",
        "t2=np.array([0])\n",
        "learn=0.001\n",
        "T=25000\n",
        "gradient_descent2(t1,t2,T,learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owNur7hZvK0x"
      },
      "source": [
        "Σε περίπτωση που αρχικοποιήσουμε τις τιμές x1,x2 με αριθμό διαφορετικό του μηδενός, προκύπτει πρόβλημα όπου οι μεταβλητές φεύγουν εκτός ορίων, ακόμη και με learning rate το οποίο σε κανονικές συνθήκες (x1,x2 αρχικοποιημένα στο 0) εκτελείται για μεγάλο αριθμό επαναλήψεων"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhLiHKCzuGZ2"
      },
      "outputs": [],
      "source": [
        "t1=np.array([20])\n",
        "t2=np.array([45])\n",
        "learn=0.0001\n",
        "T=1000\n",
        "gradient_descent2(t1,t2,T,learn)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Georgantopoulos_sdi1900036.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
